{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvolutionalNeuralNetwork(Version2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ectv0tGJQYR"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "\n",
        "p_camelyon, p_camelyon_info = tfds.load(\"patch_camelyon\", with_info=True)"
      ],
      "metadata": {
        "id": "WZ0Ng352JdGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a \"normalize\" function to process the data before feeding it into the deep neural network. \n",
        "\n",
        "def normalize(x):\n",
        "  image, label = x['image'], x['label']\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  label = tf.one_hot(label, 2, dtype=tf.float32)\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "LQXyvhM_Jdup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying \"normalize\" function along with others to the data\n",
        "\n",
        "# Processing the training dataset\n",
        "train_ds = p_camelyon['train'].map(normalize, num_parallel_calls=8)\n",
        "train_ds = train_ds.shuffle(1024)\n",
        "train_ds = train_ds.repeat()\n",
        "train_ds = train_ds.batch(32)\n",
        "train_ds = train_ds.prefetch(2)\n",
        "\n",
        "# Processing validation dataset\n",
        "validation_ds = p_camelyon['validation'].map(normalize, num_parallel_calls=8)\n",
        "validation_ds = validation_ds.repeat()\n",
        "validation_ds = validation_ds.batch(128)\n",
        "validation_ds = validation_ds.prefetch(2)\n",
        "\n",
        "#Processing the test dataset\n",
        "test_ds = p_camelyon['test'].map(normalize, num_parallel_calls=8)\n",
        "test_ds = test_ds.batch(128)\n",
        "test_ds = test_ds.prefetch(2)\n",
        "\n",
        "#Seperating image and label into different variables\n",
        "train_images, train_labels = next(iter(train_ds))\n",
        "valid_images, valid_labels = next(iter(validation_ds))\n",
        "test_images, test_labels  = next(iter(test_ds))\n",
        "\n"
      ],
      "metadata": {
        "id": "Eh6OtG3hJgpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "batch_size_for_training = 32\n",
        "batch_size_for_validation = 128\n",
        "batch_size_for_test = 128\n",
        "\n",
        "training_size = 262144\n",
        "validation_size = 32768\n",
        "test_size =  32768\n",
        "        \n",
        "\n",
        "# Calculate steps for training and testing the model\n",
        "calculate_steps_training = lambda x: int(math.ceil(1. * x / batch_size_for_training))\n",
        "calculate_steps_validation = lambda x: int(math.ceil(1. * x / batch_size_for_validation))\n",
        "calculate_steps_test= lambda x: int(math.ceil(1. * x / batch_size_for_test))\n",
        "\n",
        "steps_per_epoch = calculate_steps_training(training_size) \n",
        "validation_steps = calculate_steps_validation(validation_size)\n",
        "steps = calculate_steps_test(test_size)\n",
        "\n",
        "print(\"Training steps: \", steps_per_epoch)\n",
        "print(\"Validation steps: \", validation_steps)\n",
        "print(\"Testing steps: \", steps)\n"
      ],
      "metadata": {
        "id": "n4EcWHsGJ7Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Convolutonal Neural Network Version 2 Model 1\n",
        "\n",
        "# Setting up the output with the right size\n",
        "input = Input(shape=(96,96,3))\n",
        "\n",
        "# Rely activation functions only, with softmax activation function for the last Dense layer\n",
        "x = Conv2D(16, (3,3), activation='relu', padding='valid')(input)\n",
        "x = Conv2D(16, (3,3), activation='relu', padding='valid')(input)\n",
        "x = MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='valid')(x)\n",
        "x = Conv2D(16, (3,3), activation='relu', padding='valid')(input)\n",
        "x = MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='valid')(x)\n",
        "x = Conv2D(16, (3,3), activation='relu', padding='valid')(input)\n",
        "x = MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(rate=0.3)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "# Optimizer = adam\n",
        "# Loss function = binary_cross_entropy\n",
        "model_1_version_2 = Model(inputs=input, outputs = output)\n",
        "model_1_version_2.compile(optimizer='adam', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model_1_version_2.summary()\n",
        "plot_model(model_1_version_2)"
      ],
      "metadata": {
        "id": "I3Ph9AcvJka0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_hist(history):\n",
        "  plt.plot(history.history[\"acc\"])\n",
        "  plt.plot(history.history[\"val_acc\"])\n",
        "  plt.title(\"Model Accuracy\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "XD5CXsXEKaK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "\n",
        "history_1_version_2= model_1_version_2.fit(train_ds, validation_data=validation_ds, epochs = 10, verbose = 1, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps)"
      ],
      "metadata": {
        "id": "sIuxAt5AKeJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_hist(history_1_version_2)"
      ],
      "metadata": {
        "id": "Nc9gClwfbDSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the model\n",
        "\n",
        "print(\"Test dataset accuracy for model 1 version 2 is: {0:.4f}\".format(model_1_version_2.evaluate(test_ds, steps=steps, verbose=1)[1]))"
      ],
      "metadata": {
        "id": "lqArju91cRXA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}